{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conditional-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    DataProcessor,\n",
    "    InputExample,\n",
    "    glue_convert_examples_to_features,\n",
    ")\n",
    "from tqdm import tqdm_notebook, trange, tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-orleans",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impressive-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = {\n",
    "    \"save_results_path\": 'outputs',\n",
    "    \"pretrain_dir\": 'models',\n",
    "    \"bert_model\": \"/fred/oz064/xcai/paper1/pytorch/huggingface/bert-base-uncased\",\n",
    "    \"max_seq_length\": None,\n",
    "    \"feat_dim\": 768,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"freeze_bert_parameters\": True,\n",
    "    \"save_model\": True,\n",
    "    \"save_results\": True,\n",
    "    \"dataset\": \"oos\",\n",
    "    \"known_cls_ratio\": 0.75,\n",
    "    \"labeled_ratio\": 1.0,\n",
    "    \"method\": None,\n",
    "    \"seed\": 0,\n",
    "    \"gpu_id\": '0',\n",
    "    \"lr\": 2e-5,\n",
    "    \"num_train_epochs\": 100.0,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"eval_batch_size\": 64,\n",
    "    \"wait_patient\": 10,\n",
    "    \"lr_boundary\": 0.05,\n",
    "    \"num_labels\": 10,\n",
    "}\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-trauma",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/data_full.json'\n",
    "def data_read(data_path):\n",
    "    reader = []\n",
    "    with open (data_path) as f:\n",
    "        reader = json.load(f)\n",
    "    return reader      \n",
    "data_read(data_path).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "empirical-cambodia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['translate', 'transfer', 'timer', 'definition', 'meaning_of_life', 'insurance_change', 'find_phone', 'travel_alert', 'pto_request', 'improve_credit_score', 'fun_fact', 'change_language', 'payday', 'replacement_card_duration', 'time', 'application_status', 'flight_status', 'flip_coin', 'change_user_name', 'where_are_you_from', 'shopping_list_update', 'what_can_i_ask_you', 'maybe', 'oil_change_how', 'restaurant_reservation', 'balance', 'confirm_reservation', 'freeze_account', 'rollover_401k', 'who_made_you', 'distance', 'user_name', 'timezone', 'next_song', 'transactions', 'restaurant_suggestion', 'rewards_balance', 'pay_bill', 'spending_history', 'pto_request_status', 'credit_score', 'new_card', 'lost_luggage', 'repeat', 'mpg', 'oil_change_when', 'yes', 'travel_suggestion', 'insurance', 'todo_list_update', 'reminder', 'change_speed', 'tire_pressure', 'no', 'apr', 'nutrition_info', 'calendar', 'uber', 'calculator', 'date', 'carry_on', 'pto_used', 'schedule_maintenance', 'travel_notification', 'sync_device', 'thank_you', 'roll_dice', 'food_last', 'cook_time', 'reminder_update', 'report_lost_card', 'ingredient_substitution', 'make_call', 'alarm', 'todo_list', 'change_accent', 'w2', 'bill_due', 'calories', 'damaged_card', 'restaurant_reviews', 'routing', 'do_you_have_pets', 'schedule_meeting', 'gas_type', 'plug_type', 'tire_change', 'exchange_rate', 'next_holiday', 'change_volume', 'who_do_you_work_for', 'credit_limit', 'how_busy', 'accept_reservations', 'order_status', 'pin_change', 'goodbye', 'account_blocked', 'what_song', 'international_fees', 'last_maintenance', 'meeting_schedule', 'ingredients_list', 'report_fraud', 'measurement_conversion', 'smart_home', 'book_hotel', 'current_location', 'weather', 'taxes', 'min_payment', 'whisper_mode', 'cancel', 'international_visa', 'vaccines', 'pto_balance', 'directions', 'spelling', 'greeting', 'reset_settings', 'what_is_your_name', 'direct_deposit', 'interest_rate', 'credit_limit_change', 'what_are_your_hobbies', 'book_flight', 'shopping_list', 'text', 'bill_balance', 'share_location', 'redeem_rewards', 'play_music', 'calendar_update', 'are_you_a_bot', 'gas', 'expiration_date', 'update_playlist', 'cancel_reservation', 'tell_joke', 'change_ai_name', 'how_old_are_you', 'car_rental', 'jump_start', 'meal_suggestion', 'recipe', 'income', 'order', 'traffic', 'order_checks', 'card_declined', 'oos']\n"
     ]
    }
   ],
   "source": [
    "# data generation\n",
    "train_data = data_read(data_path)[\"train\"]\n",
    "val_data = data_read(data_path)[\"val\"]\n",
    "test_data = data_read(data_path)[\"test\"]\n",
    "oos_train_data = data_read(data_path)[\"oos_train\"]\n",
    "oos_val_data = data_read(data_path)[\"oos_val\"]\n",
    "oos_test_data = data_read(data_path)[\"oos_test\"]\n",
    "\n",
    "# data label generation\n",
    "def label_generator(train_data, oos_train_data):\n",
    "    data_label = []\n",
    "    for index in range(0,len(train_data)) :\n",
    "        if train_data[index][1] not in data_label:\n",
    "            data_label.append(train_data[index][1])\n",
    "            index = index + 1\n",
    "    data_label.append(oos_train_data[0][1])\n",
    "    return data_label\n",
    "idx_to_type = label_generator(train_data, oos_train_data)\n",
    "print(idx_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "requested-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InputExample(guid='0', text_a=train_data[0][0], label=train_data[0][1])\n",
    "def create_examples(data):\n",
    "    examples = []\n",
    "    for i, e in enumerate(data):\n",
    "        examples.append(InputExample(guid = str(i), text_a=e[0], label=e[1]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unnecessary-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[InputExample(guid='0', text_a='what expression would i use to say i love you if i were an italian', text_b=None, label='translate'), InputExample(guid='1', text_a=\"can you tell me how to say 'i do not speak much spanish', in spanish\", text_b=None, label='translate'), InputExample(guid='2', text_a=\"what is the equivalent of, 'life is good' in french\", text_b=None, label='translate')]\n"
     ]
    }
   ],
   "source": [
    "examples = create_examples(train_data)\n",
    "print(examples[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agreed-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(tokenizer, data_path):\n",
    "    def generate_dataloader_inner(examples, data_type='train'):\n",
    "        features = glue_convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            label_list = idx_to_type,\n",
    "            max_length = 64,\n",
    "            output_mode = 'classification'\n",
    "        )\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.LongTensor([f.input_ids for f in features]),\n",
    "            torch.LongTensor([f.attention_mask for f in features]),\n",
    "            torch.LongTensor([f.token_type_ids for f in features]),\n",
    "            torch.LongTensor([f.label for f in features])   \n",
    "        )\n",
    "        if data_type == 'train':\n",
    "            sampler = torch.utils.data.RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, sampler = sampler, batch_size = 32\n",
    "        )\n",
    "        return dataloader\n",
    "    \n",
    "    # notice here class OOS is always the last label\n",
    "    train_examples = create_examples(data_read(data_path)[\"train\"]+ data_read(data_path)[\"oos_train\"])\n",
    "    print('Load Example Finish')\n",
    "    train_loader = generate_dataloader_inner(train_examples, data_type='train')\n",
    "    print('Generate DataLoader Finish')\n",
    "\n",
    "    valid_examples = create_examples(data_read(data_path)[\"val\"] + data_read(data_path)[\"oos_val\"])\n",
    "    print('Load Example Finish')\n",
    "    valid_loader = generate_dataloader_inner(valid_examples, data_type='valid')\n",
    "    print('Generate DataLoader Finish')   \n",
    "\n",
    "    test_examples = create_examples(data_read(data_path)[\"test\"] + data_read(data_path)[\"oos_test\"])\n",
    "    print('Load Example Finish')\n",
    "    test_loader = generate_dataloader_inner(test_examples, data_type='valid')\n",
    "    print('Generate DataLoader Finish')\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heard-clark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Example Finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/skylake/software/Transformers/4.3.3-gni-2020.0-Python-3.8.5/lib/python3.8/site-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate DataLoader Finish\n",
      "Load Example Finish\n",
      "Generate DataLoader Finish\n",
      "Load Example Finish\n",
      "Generate DataLoader Finish\n"
     ]
    }
   ],
   "source": [
    "bert_path = \"/fred/oz064/xcai/paper1/pytorch/huggingface/bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "train_loader, valid_loader, test_loader = generate_dataloaders(tokenizer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "electrical-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in valid_loader:\n",
    "# for batch in train_loader:\n",
    "#     print(batch[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-cemetery",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frequent-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(x, y, num_cls, w, \n",
    "             reuse=False, alpha=0.35, beta=0.35, scale=64, \n",
    "             lamb1=1, lamb2=10, ce_loss_oos=False, name='cos_margin_loss'):\n",
    "    '''\n",
    "    x: B x D - features\n",
    "    y: B - labels\n",
    "    num_cls: 1 - total class number, the last cls being out of scope\n",
    "    w: num_cls x D - mean feature vectors (centroids)\n",
    "    alpah: 1 - in scope margin\n",
    "    beta: 1 - out of scope margin\n",
    "    scale: 1 - scaling paramter\n",
    "    lamb1: weight of 1-cosine\n",
    "    lamb2: weight of max\n",
    "    ce_loss_oos: calculate oos loss in cross entropy\n",
    "    ''' \n",
    "    #normalize the feature and weight\n",
    "    #(B,D)\n",
    "    x_feat_norm = F.normalize(x,p=2,dim=1,eps=1e-12)\n",
    "    #(D,num_cls)\n",
    "    w_feat_norm = torch.transpose(F.normalize(w,p=2,dim=1,eps=1e-12), 0, 1)\n",
    "\n",
    "    # get the scores after normalization \n",
    "    #(B,num_cls)\n",
    "    xw_norm = torch.matmul(x_feat_norm, w_feat_norm)  # cosine similarity\n",
    "\n",
    "    # xbj's loss, first row, adjust the cosine similarity by a margin, only apply to in-scope instances\n",
    "    xw_norm[:, :-1] -= alpha #(B,num_cls)\n",
    "#     xw_norm[:, -1] -= alpha #(B,num_cls)\n",
    "\n",
    "    # margin based softmax loss\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    ce_loss = loss_fn(xw_norm, y)\n",
    "    if not ce_loss_oos:\n",
    "        ce_loss[y == 150] = 0\n",
    "#     print(\"ce_loss shape: \", ce_loss.shape)\n",
    "    \n",
    "    # xbj loss, second row, only applies to out of scope instances\n",
    "    out_of_scope_loss_part2 = torch.max(xw_norm[:, :-1] - alpha, dim=1)[0] - xw_norm[:, -1]\n",
    "    out_of_scope_loss_part2[out_of_scope_loss_part2 < 0] = 0\n",
    "#     print(\"out_of_scope_loss_part2 shape: \", out_of_scope_loss_part2.shape)\n",
    "    out_of_scope_loss = lamb1 * (1 - xw_norm[:, -1]) + lamb2 * out_of_scope_loss_part2\n",
    "#     print(\"out_of_scope_loss shape: \", out_of_scope_loss.shape)\n",
    "       \n",
    "    out_of_scope_loss[y < 150] = 0\n",
    "    \n",
    "    loss = torch.mean(ce_loss + out_of_scope_loss)\n",
    "    \n",
    "    return loss \n",
    "\n",
    "def predict(x, w, alpha=0.35):\n",
    "    '''\n",
    "    x: B x D - features\n",
    "    w: num_cls x D - mean feature vectors (centroids)\n",
    "    ''' \n",
    "    #normalize the feature and weight\n",
    "    #(B,D)\n",
    "#     print(\"x.size():\", x.size())\n",
    "    x_feat_norm = F.normalize(x,p=2,dim=1,eps=1e-12)\n",
    "    #(D,num_cls)\n",
    "    w_feat_norm = torch.transpose(F.normalize(w,p=2,dim=1,eps=1e-12), 0, 1)\n",
    "\n",
    "    # get the scores after normalization \n",
    "    #(B,num_cls)\n",
    "    xw_norm = torch.matmul(x_feat_norm, w_feat_norm)  # cosine similarity\n",
    "\n",
    "    xw_norm[:, :-1] -= alpha\n",
    "    \n",
    "    preds = xw_norm.max(1)[1]\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medieval-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cognitive-collapse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(args.bert_model)\n",
    "def get_optimizer(bert_model, args):\n",
    "    param_optimizer = list(bert_model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr = args.lr)   \n",
    "    return optimizer\n",
    "optimizer = get_optimizer(bert_model, args)\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id           \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "former-ghana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids...\n"
     ]
    }
   ],
   "source": [
    "def compute_centroids(dataloader, bert_model):\n",
    "    print(\"Computing centroids...\")\n",
    "    vectors = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            token_type_ids = token_type_ids.to(DEVICE)\n",
    "            outputs = bert_model(input_ids, attention_mask, token_type_ids)\n",
    "            pooler_output = outputs.pooler_output\n",
    "            vectors.append(pooler_output.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    vectors = torch.cat(vectors, 0) # num_ins, feature_dim\n",
    "    labels = torch.cat(all_labels, 0) # num_ins\n",
    "    w = []\n",
    "    for i in range(151):\n",
    "        w.append(vectors[labels==i].mean(0, keepdim=True))\n",
    "    w = torch.cat(w, 0)\n",
    "    return w\n",
    "w = compute_centroids(valid_loader, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "muslim-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.detach().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecological-career",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governmental-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloder, w, alpha=0.35):\n",
    "    w = w.to(DEVICE)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloder):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            token_type_ids = token_type_ids.to(DEVICE)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = bert_model(input_ids, attention_mask, token_type_ids)\n",
    "            pooler_output  = outputs.pooler_output \n",
    "\n",
    "            preds = predict(pooler_output, w, alpha=alpha).cpu()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "    preds = torch.cat(all_preds)\n",
    "    labels = torch.cat(all_labels)\n",
    "    accuracy = torch.sum(preds == labels).item() / labels.shape[0]\n",
    "    \n",
    "    all_recalls = [torch.sum(preds[labels==i] == labels[labels==i]).item() / torch.sum(labels == i).item() for i in range(150)]\n",
    "    out_of_scope_recall = torch.sum(preds[labels==150] == labels[labels==150]).item() / torch.sum(labels == 150).item()\n",
    "    out_of_scope_precision = torch.sum(preds[labels==150] == labels[labels==150]).item() / torch.sum(preds == 150).item()\n",
    "    in_scope_accuracy = torch.sum(preds[labels<150] == labels[labels<150]).item() / torch.sum(labels < 150).item()\n",
    "    \n",
    "    metrics = {\"accuracy\": accuracy, \n",
    "               \"out_of_scope_recall\": out_of_scope_recall, \n",
    "               \"out_of_scope_precision\": out_of_scope_precision,\n",
    "               \"in_scope_accuracy\": in_scope_accuracy,\n",
    "               \"all_recalls\": all_recalls,}\n",
    "    \n",
    "#     print(\"accuracy: \", accuracy, \"out of scope recall: \", out_of_scope_recall, \"out of scope precision: \", out_of_scope_precision)\n",
    "#     print(\"all_recalls: \", all_recalls)\n",
    "#     print(\"in_scope_accuracy: \", in_scope_accuracy)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-submission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids...\n",
      "[1] loss: 4.936\n",
      "valid:  {'accuracy': 0.07161290322580645, 'out_of_scope_recall': 0.02, 'out_of_scope_precision': 0.16666666666666666, 'in_scope_accuracy': 0.07333333333333333, 'all_recalls': [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.95, 0.0, 0.05, 0.0, 0.45, 0.0, 0.0, 0.15, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0, 0.05, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.05, 0.15, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.15, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 1.0, 0.0, 0.3, 0.05, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.05, 0.2, 0.55, 0.1, 0.0, 0.0, 0.0, 0.2, 0.05, 0.7, 0.75, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.1, 0.25, 0.0, 0.0, 0.2, 0.0, 0.3, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.05, 0.15, 0.0, 0.0, 0.0, 0.15, 0.0]}\n",
      "test:  {'accuracy': 0.06563636363636363, 'out_of_scope_recall': 0.006, 'out_of_scope_precision': 0.21428571428571427, 'in_scope_accuracy': 0.07888888888888888, 'all_recalls': [0.36666666666666664, 0.0, 0.0, 0.03333333333333333, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03333333333333333, 0.0, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.03333333333333333, 0.0, 0.0, 0.36666666666666664, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.13333333333333333, 0.3, 0.4666666666666667, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.23333333333333334, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.16666666666666666, 0.06666666666666667, 0.0, 0.0, 0.0, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.03333333333333333, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.4, 0.0, 0.06666666666666667, 0.0, 0.03333333333333333, 0.0, 0.0, 0.0, 0.06666666666666667, 0.23333333333333334, 0.5, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7333333333333333, 0.6666666666666666, 0.13333333333333333, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.1, 0.06666666666666667, 0.0, 0.43333333333333335, 0.0, 0.16666666666666666, 0.06666666666666667, 0.0, 0.0, 0.03333333333333333, 0.0, 0.23333333333333334, 0.0, 0.03333333333333333, 0.0, 0.26666666666666666, 0.0, 0.0, 0.0, 0.1, 0.0]}\n",
      "Computing centroids...\n",
      "[2] loss: 4.220\n",
      "valid:  {'accuracy': 0.84, 'out_of_scope_recall': 0.13, 'out_of_scope_precision': 0.9285714285714286, 'in_scope_accuracy': 0.8636666666666667, 'all_recalls': [0.95, 0.75, 1.0, 0.9, 0.9, 1.0, 0.8, 0.9, 1.0, 0.95, 0.95, 1.0, 0.25, 0.6, 0.95, 1.0, 1.0, 1.0, 0.45, 0.9, 0.95, 1.0, 0.9, 0.85, 0.55, 0.9, 0.95, 0.95, 1.0, 1.0, 0.9, 0.8, 0.8, 0.1, 0.7, 0.8, 0.8, 0.65, 0.9, 0.8, 0.6, 0.95, 1.0, 0.75, 0.85, 1.0, 0.8, 0.6, 0.95, 1.0, 0.4, 1.0, 1.0, 0.75, 0.9, 0.75, 0.95, 1.0, 1.0, 1.0, 1.0, 0.05, 0.95, 0.75, 0.9, 1.0, 0.95, 0.75, 0.75, 0.75, 0.25, 0.7, 0.95, 1.0, 0.9, 0.75, 0.95, 0.95, 1.0, 0.95, 0.85, 1.0, 0.95, 1.0, 0.85, 0.85, 0.8, 0.9, 0.9, 1.0, 0.75, 0.9, 1.0, 0.55, 0.65, 0.85, 0.9, 0.8, 1.0, 1.0, 0.65, 0.95, 0.7, 0.9, 0.95, 0.95, 0.7, 0.75, 1.0, 0.95, 0.8, 1.0, 0.85, 1.0, 0.6, 0.95, 1.0, 0.95, 0.95, 1.0, 1.0, 1.0, 0.9, 0.95, 0.9, 1.0, 1.0, 0.8, 0.9, 1.0, 1.0, 0.85, 1.0, 0.8, 0.9, 0.65, 0.95, 0.95, 1.0, 0.95, 0.95, 0.75, 0.95, 0.8, 1.0, 0.9, 0.7, 0.95, 0.9, 1.0]}\n",
      "test:  {'accuracy': 0.7161818181818181, 'out_of_scope_recall': 0.082, 'out_of_scope_precision': 0.9761904761904762, 'in_scope_accuracy': 0.8571111111111112, 'all_recalls': [0.9333333333333333, 0.8333333333333334, 0.9333333333333333, 0.8666666666666667, 1.0, 0.8666666666666667, 0.7, 0.9, 0.7666666666666667, 0.7666666666666667, 0.9666666666666667, 0.9, 0.23333333333333334, 0.6666666666666666, 0.9333333333333333, 1.0, 1.0, 1.0, 0.26666666666666666, 0.9666666666666667, 0.9333333333333333, 0.9333333333333333, 0.9, 0.8, 0.7666666666666667, 0.9666666666666667, 0.9, 0.9666666666666667, 0.9666666666666667, 1.0, 0.8333333333333334, 0.9, 0.8, 0.3, 0.7, 0.9, 0.7666666666666667, 0.8666666666666667, 0.7333333333333333, 0.8, 0.7333333333333333, 0.9333333333333333, 0.9666666666666667, 0.8, 1.0, 0.9666666666666667, 0.8333333333333334, 0.7666666666666667, 1.0, 0.9333333333333333, 0.4666666666666667, 0.8333333333333334, 1.0, 0.9, 0.9333333333333333, 1.0, 0.8666666666666667, 1.0, 0.9333333333333333, 0.9333333333333333, 0.9, 0.23333333333333334, 0.7666666666666667, 0.6, 0.8666666666666667, 1.0, 0.9, 0.9, 0.6333333333333333, 0.7333333333333333, 0.43333333333333335, 0.9333333333333333, 0.8666666666666667, 1.0, 0.9333333333333333, 0.8, 0.8666666666666667, 0.8333333333333334, 0.9666666666666667, 0.9333333333333333, 0.5333333333333333, 1.0, 1.0, 0.9666666666666667, 0.9666666666666667, 0.9, 0.7666666666666667, 0.9666666666666667, 1.0, 0.7333333333333333, 0.6333333333333333, 0.8, 0.7333333333333333, 0.8333333333333334, 0.9333333333333333, 0.6666666666666666, 0.7666666666666667, 0.8333333333333334, 1.0, 0.8666666666666667, 0.9, 1.0, 0.6, 0.7333333333333333, 0.9666666666666667, 0.8333333333333334, 0.6333333333333333, 0.9333333333333333, 1.0, 1.0, 0.8333333333333334, 0.9, 0.9666666666666667, 1.0, 0.9333333333333333, 0.9, 1.0, 0.8666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333, 0.9, 0.8, 0.9666666666666667, 1.0, 1.0, 0.8666666666666667, 0.9333333333333333, 0.7333333333333333, 0.9, 0.8666666666666667, 0.8666666666666667, 0.9666666666666667, 0.8666666666666667, 0.9666666666666667, 0.9, 1.0, 1.0, 1.0, 0.9666666666666667, 0.9, 0.5666666666666667, 1.0, 0.5666666666666667, 0.9666666666666667, 0.8333333333333334, 0.7, 1.0, 1.0, 0.6333333333333333]}\n",
      "Computing centroids...\n",
      "[3] loss: 4.132\n",
      "valid:  {'accuracy': 0.8983870967741936, 'out_of_scope_recall': 0.1, 'out_of_scope_precision': 1.0, 'in_scope_accuracy': 0.925, 'all_recalls': [1.0, 0.9, 1.0, 0.95, 0.9, 0.95, 0.85, 0.9, 1.0, 0.85, 1.0, 1.0, 0.75, 0.75, 0.95, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 1.0, 0.85, 1.0, 0.7, 0.95, 0.95, 0.95, 1.0, 0.95, 1.0, 0.85, 0.5, 0.9, 0.75, 0.85, 1.0, 0.85, 0.95, 0.8, 0.9, 0.85, 1.0, 0.8, 1.0, 1.0, 0.85, 0.85, 0.95, 1.0, 0.6, 1.0, 1.0, 0.85, 1.0, 0.55, 0.95, 1.0, 1.0, 1.0, 1.0, 0.8, 0.95, 0.95, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.8, 0.8, 0.9, 1.0, 0.95, 0.8, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 0.9, 0.95, 0.9, 0.95, 1.0, 1.0, 1.0, 1.0, 0.65, 0.65, 1.0, 1.0, 0.85, 1.0, 1.0, 0.6, 0.95, 1.0, 0.9, 0.95, 1.0, 0.9, 0.85, 0.95, 0.95, 0.8, 1.0, 0.85, 1.0, 0.85, 0.85, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 0.95, 0.95, 0.9, 1.0, 1.0, 0.85, 0.9, 1.0, 0.95, 0.9, 0.95, 0.9, 0.95, 0.65, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.9, 1.0, 0.95, 0.75, 0.95, 0.9, 1.0]}\n",
      "test:  {'accuracy': 0.7694545454545455, 'out_of_scope_recall': 0.096, 'out_of_scope_precision': 0.9795918367346939, 'in_scope_accuracy': 0.9191111111111111, 'all_recalls': [1.0, 0.9, 0.9333333333333333, 0.9333333333333333, 0.9666666666666667, 0.9, 0.8333333333333334, 0.9666666666666667, 0.8333333333333334, 0.8, 1.0, 0.9666666666666667, 0.8666666666666667, 0.8333333333333334, 0.9333333333333333, 1.0, 1.0, 1.0, 0.8, 1.0, 0.9333333333333333, 1.0, 0.9, 0.9666666666666667, 0.9333333333333333, 1.0, 0.9, 0.9666666666666667, 0.9666666666666667, 1.0, 0.8666666666666667, 0.9, 0.6333333333333333, 0.9, 0.7333333333333333, 0.9, 0.9666666666666667, 0.8666666666666667, 0.7666666666666667, 0.8333333333333334, 0.9333333333333333, 0.9666666666666667, 0.9666666666666667, 0.8666666666666667, 1.0, 0.9666666666666667, 0.8333333333333334, 0.9666666666666667, 1.0, 1.0, 0.7666666666666667, 0.9666666666666667, 1.0, 0.9666666666666667, 1.0, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333, 0.8666666666666667, 0.8333333333333334, 0.9, 0.9333333333333333, 1.0, 1.0, 0.9, 0.8666666666666667, 0.8, 0.5666666666666667, 0.9333333333333333, 0.9, 0.9666666666666667, 1.0, 0.9, 1.0, 0.8333333333333334, 0.9666666666666667, 0.9333333333333333, 0.7333333333333333, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9666666666666667, 0.9666666666666667, 1.0, 0.8333333333333334, 0.7, 0.9, 0.7666666666666667, 0.8666666666666667, 0.9666666666666667, 0.8333333333333334, 0.9, 0.9666666666666667, 1.0, 0.9, 0.7, 1.0, 0.7666666666666667, 0.8333333333333334, 1.0, 0.9666666666666667, 1.0, 1.0, 1.0, 0.9333333333333333, 0.9, 0.8666666666666667, 0.9333333333333333, 1.0, 0.9666666666666667, 0.7666666666666667, 1.0, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333, 0.9666666666666667, 0.9, 0.9666666666666667, 1.0, 0.9333333333333333, 0.8666666666666667, 0.9333333333333333, 0.8, 0.9333333333333333, 0.7666666666666667, 0.8666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333, 1.0, 1.0, 1.0, 0.9666666666666667, 0.9333333333333333, 0.9666666666666667, 1.0, 0.7, 0.9666666666666667, 0.8333333333333334, 0.7333333333333333, 1.0, 1.0, 0.8]}\n",
      "Computing centroids...\n"
     ]
    }
   ],
   "source": [
    "LAMB1 = 0\n",
    "LAMB2 = 0\n",
    "CE_LOSS_OOS = True\n",
    "ALPHA = 0.0\n",
    "ALPHA_EVAL = 0.0\n",
    "CENTROID_UPDATE_EPOCH = 40\n",
    "CHECKPOINT_PATH = \"checkpoints/alpha_{}_alpha_eval_{}_lamb1_{}_lamb2_{}_celossoos_{}_update_centroids_epoch_{}\".format(ALPHA, \n",
    "    ALPHA_EVAL, LAMB1, LAMB2, \n",
    "    CE_LOSS_OOS, CENTROID_UPDATE_EPOCH)\n",
    "!mkdir -p $CHECKPOINT_PATH\n",
    "\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    if epoch < CENTROID_UPDATE_EPOCH:\n",
    "        w = compute_centroids(train_loader, bert_model).detach().to(DEVICE)\n",
    "    last_w = w.cpu()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        token_type_ids = token_type_ids.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = bert_model(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output  = outputs.pooler_output \n",
    "#         print(\"y.size():\", y.size())\n",
    "        \n",
    "        loss = cos_loss(pooler_output, labels, 151, w, alpha=ALPHA, \n",
    "                        lamb1=LAMB1, lamb2=LAMB2, ce_loss_oos=CE_LOSS_OOS,\n",
    "                        beta=0.35, scale=64, name='cos_margin_loss')\n",
    "#         print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss / (i+1)))\n",
    "    metrics = evaluate(valid_loader, last_w, alpha=ALPHA_EVAL)\n",
    "    print(\"valid: \", metrics)\n",
    "    with open(os.path.join(CHECKPOINT_PATH, \"metrics.txt\"), \"a\") as metrics_out:\n",
    "        metrics_out.write(\"epoch {}\\n\".format(epoch+1) + str(metrics) + \"\\n\")\n",
    "    metrics = evaluate(test_loader, last_w, alpha=ALPHA_EVAL)\n",
    "    print(\"test: \", metrics)\n",
    "    with open(os.path.join(CHECKPOINT_PATH, \"test_metrics.txt\"), \"a\") as metrics_out:\n",
    "        metrics_out.write(\"epoch {}\\n\".format(epoch+1) + str(metrics) + \"\\n\")\n",
    "    \n",
    "print('Finished Training')\n",
    "bert_model.save_pretrained(CHECKPOINT_PATH)\n",
    "np.savetxt(os.path.join(CHECKPOINT_PATH, \"last_w.txt\"), last_w.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = BertModel.from_pretrained(CHECKPOINT_PATH)\n",
    "# bert_model.to(DEVICE)\n",
    "# metrics = evaluate(test_loader, last_w)\n",
    "# print(metrics)\n",
    "# with open(os.path.join(CHECKPOINT_PATH, \"test_metrics.txt\"), \"a\") as metrics_out:\n",
    "#     metrics_out.write(str(metrics) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model.save_pretrained(CHECKPOINT_PATH)\n",
    "# np.savetxt(os.path.join(CHECKPOINT_PATH, \"last_w.txt\"), last_w.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-utility",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-creek",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib import pyplot as plt\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# X_2d = tsne.fit_transform(last_w.numpy())\n",
    "\n",
    "# target_ids = range(len(idx_to_type))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# for i, label in zip(target_ids, idx_to_type):\n",
    "#     plt.scatter(X_2d[i, 0], X_2d[i, 1], label=label)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_for_visualization(dataloder, w):\n",
    "#     w = w.to(DEVICE)\n",
    "#     all_labels = []\n",
    "#     all_vectors = []\n",
    "#     with torch.no_grad():\n",
    "#         for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloder):\n",
    "#             input_ids = input_ids.to(DEVICE)\n",
    "#             attention_mask = attention_mask.to(DEVICE)\n",
    "#             token_type_ids = token_type_ids.to(DEVICE)\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = bert_model(input_ids, attention_mask, token_type_ids)\n",
    "#             pooler_output  = outputs.pooler_output \n",
    "\n",
    "#             all_vectors.append(pooler_output)\n",
    "#             all_labels.append(labels)\n",
    "            \n",
    "#     labels = torch.cat(all_labels)\n",
    "#     vectors = torch.cat(all_vectors, 0)\n",
    "    \n",
    "#     return labels.cpu().numpy(), vectors.cpu().numpy()\n",
    "# labels, vectors = predict_for_visualization(train_loader, last_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors.shape\n",
    "# vectors_and_weights = np.concatenate([vectors, last_w.numpy()], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors_and_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-conservation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib import pyplot as plt\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# X_2d = tsne.fit_transform(vectors_and_weights)\n",
    "\n",
    "# target_ids = range(len(idx_to_type[:]))\n",
    "# X_2d_vectors = X_2d[:-151]\n",
    "# X_2d_w = X_2d[-151:]\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# for i, label in zip(target_ids, idx_to_type[:]):\n",
    "#     if i == 150:\n",
    "#         plt.scatter(X_2d_vectors[labels==i, 0], X_2d_vectors[labels==i, 1], c='k', label=label)\n",
    "#         plt.scatter(X_2d_w[i, 0], X_2d_w[i, 1], c='k', label=label+\"_w\")\n",
    "#     elif i > 130:\n",
    "#         plt.scatter(X_2d_vectors[labels==i, 0], X_2d_vectors[labels==i, 1], label=label)\n",
    "#         plt.scatter(X_2d_w[i, 0], X_2d_w[i, 1], label=label+\"_w\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
